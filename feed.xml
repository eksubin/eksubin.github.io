<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://eksubin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://eksubin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-12T20:12:31+00:00</updated><id>https://eksubin.github.io/feed.xml</id><title type="html">blank</title><subtitle>Portfolio of Mr Subin Erattakulangara </subtitle><entry><title type="html">MonaiLabelLite - A lightweight implementation of MonaiLabel for Docker</title><link href="https://eksubin.github.io/blog/2025/2025/" rel="alternate" type="text/html" title="MonaiLabelLite - A lightweight implementation of MonaiLabel for Docker"/><published>2025-02-11T00:00:00+00:00</published><updated>2025-02-11T00:00:00+00:00</updated><id>https://eksubin.github.io/blog/2025/2025</id><content type="html" xml:base="https://eksubin.github.io/blog/2025/2025/"><![CDATA[<div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/docker_minimization.webp" sizes="95vw"/> <img src="/assets/img/docker_minimization.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Monai Label is an excellent tool, widely regarded as one of the best available for developers and researchers working on deep learning applications in medical imaging. However, the recent additions to the Monai Label repository have significantly increased its size and complexity.</p> <ul> <li>This increase generally doesn’t impact users running the software locally or utilizing a GPU on their system.</li> <li>It also adds considerable value to the open-source community.</li> </ul> <h2 id="impact-on-building-a-docker-image">Impact on Building a Docker Image</h2> <p>When building Docker images, we strive for a lighter footprint. Unfortunately, integrating the latest Monai Label into your code can lead to substantial image bloat. Personally, I sought to deploy a Monai server on CPU for a custom segmentation algorithm I developed. Yet, with the latest version of Monai Label, my Docker image ballooned to 20 GB, causing performance issues on MacBooks due to the increased overhead of running Docker Desktop. Thus, I decided to create a streamlined version of Monai Label tailored for CPU usage.</p> <p>Here are some components you can remove to achieve a lighter image:</p> <ol> <li>The SAM2 model is included by default for all applications when Python is 3.10 or higher. You can remove this if SAM functionality is not required for your code.</li> <li>If you’re not performing inference on a GPU, the GPU version of PyTorch can be omitted.</li> <li>Nvidia CUDA setup files can take up to 500 MB of space and can be excluded if not needed.</li> </ol> <p>After eliminating these components and ensuring that the CPU version of libraries like PyTorch is installed, I managed to reduce the Docker image size from <strong>20 GB to 10 GB</strong>.</p> <h2 id="my-approach-to-reducing-image-size">My Approach to Reducing Image Size</h2> <ol> <li>Created a forked repository of the original Monai Label.</li> <li>Removed SAM installation from <code class="language-plaintext highlighter-rouge">requirements.txt</code> to save storage.</li> <li>Executed the <code class="language-plaintext highlighter-rouge">requirements.txt</code> independently, which unfortunately installs the GPU version of PyTorch.</li> <li>Utilized a wheel to reinstall PyTorch and torchvision with CPU support.</li> <li>Cloned the Git repository and added it to the root path.</li> </ol> <h2 id="docker-code">Docker code</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">FROM</span> <span class="n">python</span><span class="p">:</span><span class="mf">3.10</span><span class="o">-</span><span class="n">slim</span>

<span class="n">WORKDIR</span> <span class="o">/</span><span class="n">app</span>

<span class="c1"># Install git
</span><span class="n">RUN</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">update</span> <span class="o">&amp;&amp;</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="o">-</span><span class="n">y</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">install</span><span class="o">-</span><span class="n">recommends</span> <span class="n">git</span> <span class="o">&amp;&amp;</span> <span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">apt</span><span class="o">/</span><span class="n">lists</span><span class="o">/*</span>

<span class="c1"># Clone MONAI Label Lite - No installation of SAM
</span><span class="n">RUN</span> <span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">eksubin</span><span class="o">/</span><span class="n">MONAILabelLite</span>

<span class="c1"># Install Python dependencies
</span><span class="n">RUN</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">cache</span><span class="o">-</span><span class="nb">dir</span> <span class="o">-</span><span class="n">r</span> <span class="n">MONAILabelLite</span><span class="o">/</span><span class="n">requirements</span><span class="p">.</span><span class="n">txt</span>

<span class="c1"># Set MONAILabel script paths in environment variables
</span><span class="n">ENV</span> <span class="n">PATH</span><span class="o">=</span><span class="sh">"</span><span class="s">/app/MONAILabelLite/monailabel/scripts:${PATH}</span><span class="sh">"</span>

<span class="c1"># Remove torch gpu and install torch cpu
</span><span class="n">RUN</span> <span class="n">pip</span> <span class="n">uninstall</span> <span class="o">-</span><span class="n">y</span> <span class="n">torch</span> <span class="n">torchvision</span>
<span class="n">RUN</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span> <span class="n">torchvision</span> <span class="o">--</span><span class="n">extra</span><span class="o">-</span><span class="n">index</span><span class="o">-</span><span class="n">url</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">download</span><span class="p">.</span><span class="n">pytorch</span><span class="p">.</span><span class="n">org</span><span class="o">/</span><span class="n">whl</span><span class="o">/</span><span class="n">cpu</span>

<span class="c1"># Remove the git - Not needed after the first operation
</span><span class="n">RUN</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">update</span> <span class="o">&amp;&amp;</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">remove</span> <span class="o">-</span><span class="n">y</span> <span class="n">git</span>

<span class="c1"># Copy the application code
</span><span class="n">COPY</span> <span class="p">.</span> <span class="p">.</span>
<span class="c1"># Expose the port the app runs on
</span><span class="n">EXPOSE</span> <span class="mi">8000</span>

<span class="c1"># Command to run the application
</span><span class="n">CMD</span> <span class="p">[</span><span class="sh">"</span><span class="s">add command to run your application</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Tools"/><category term="AI"/><category term="Docker"/><summary type="html"><![CDATA[Latest Monai Label is too heavy for a docker image, Let's make it lighter]]></summary></entry><entry><title type="html">Building a CLSTM cell for 4D Data</title><link href="https://eksubin.github.io/blog/2025/code/" rel="alternate" type="text/html" title="Building a CLSTM cell for 4D Data"/><published>2025-01-26T00:00:00+00:00</published><updated>2025-01-26T00:00:00+00:00</updated><id>https://eksubin.github.io/blog/2025/code</id><content type="html" xml:base="https://eksubin.github.io/blog/2025/code/"><![CDATA[<p>LSTM or Long Short-Term Memory is a specialized type of recurrent neural network (RNN) designed to address the vanishing gradient problem often encountered by traditional RNNs when processing long sequences.</p> <p>Recurrent Neural Networks (RNNs) are a type of neural network designed to work with sequential data. Unlike traditional feedforward networks, RNNs have connections that loop back on themselves, allowing them to maintain a “memory” of past inputs. This memory enables them to process information from previous steps in the sequence, making them well-suited for tasks like natural language processing, speech recognition, and time series analysis. However they are prone to vanishing gradient problem.</p> <p>LSTMs overcome this by incorporating a memory cell, which allows the network to store and access information over extended periods. Here is a pictoral representation of an LSTM Cell.</p> <div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/clstmCell-480.webp 480w,/assets/img/clstmCell-800.webp 800w,/assets/img/clstmCell-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/clstmCell.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Comparison between RNN and CLSTM Cell </div> <p>The LSTM cell consists of three gates:</p> <ul> <li><strong>Input Gate</strong>: Determines which new information should be stored in the cell.</li> <li><strong>Forget Gate</strong>: Decides which information should be discarded from the cell.</li> <li><strong>Output Gate</strong>: Controls what information from the cell is used to compute the output.</li> </ul> <p>These gates regulate the flow of information into and out of the cell, enabling the network to selectively remember or forget information.</p> <h2 id="pytorch-implementation">Pytorch implementation</h2> <p>Here is the pytorch implementation of a vanilla LSTM Cell.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">LSTMCell</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LSTMCell</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="c1"># Forget, Input, and Output gates weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Wf</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wi</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wo</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hx</span><span class="p">):</span>
        <span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span> <span class="o">=</span> <span class="n">hx</span>

        <span class="c1"># Concatenate input and previous hidden state
</span>        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Forget gate
</span>        <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wf</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Input gate
</span>        <span class="n">i_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wi</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Candidate memory
</span>        <span class="n">c_hat_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wc</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Cell state
</span>        <span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">c_hat_t</span>
        <span class="c1"># Output gate
</span>        <span class="n">o_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wo</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Hidden state
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span>
</code></pre></div></div> <p>Sample code for running the vanilla LSTM Cell.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">lstm_cell</span> <span class="o">=</span> <span class="nc">LSTMCell</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

<span class="c1"># Example input (batch_size=1, input_size=4)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
<span class="c1"># Initial hidden state and cell state
</span><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">c_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

<span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span> <span class="o">=</span> <span class="nf">lstm_cell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next hidden state:</span><span class="sh">"</span><span class="p">,</span> <span class="n">h_t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next cell state:</span><span class="sh">"</span><span class="p">,</span> <span class="n">c_t</span><span class="p">)</span>
</code></pre></div></div> <h3 id="converting-lstm-into-clstm">Converting LSTM into CLSTM</h3> <p>To convert an LSTM cell into a Convolutional LSTM (ConvLSTM), you typically use convolutional layers instead of fully connected layers for processing the input and hidden states. Below is an implementation of a ConvLSTM cell in PyTorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">ConvLSTMCell</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ConvLSTMCell</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>

        <span class="c1"># Define the convolutional layers for the gates
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Wf</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wi</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wo</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hx</span><span class="p">):</span>
        <span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span> <span class="o">=</span> <span class="n">hx</span>

        <span class="c1"># Concatenate input and previous hidden state along the channel dimension
</span>        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Forget gate
</span>        <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wf</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Input gate
</span>        <span class="n">i_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wi</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Candidate memory
</span>        <span class="n">c_hat_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wc</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Now the operation is a Convolution
</span>        <span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">c_hat_t</span>
        <span class="c1"># Output gate
</span>        <span class="n">o_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wo</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Hidden state
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span>
</code></pre></div></div> <p>Sample code to use the CLSTM cell</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_channels</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">hidden_channels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">conv_lstm_cell</span> <span class="o">=</span> <span class="nc">ConvLSTMCell</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">,</span>
                                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Padding to keep the output spatial dimensions same
</span>
<span class="c1"># Example input (batch_size=1, channels=1, height=4, width=4)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1"># Initial hidden state and cell state
</span><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">c_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span> <span class="o">=</span> <span class="nf">conv_lstm_cell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next hidden state shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">h_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next cell state shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">c_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h2 id="preparing-clstm-for-handling-3d-data">Preparing CLSTM for Handling 3D Data</h2> <p>When dealing with 3D data over time, the input shape follows the format:</p> <ul> <li><strong>b</strong>: batch size</li> <li><strong>t</strong>: number of time frames</li> <li><strong>c</strong>: number of channels</li> <li><strong>d</strong>: depth</li> <li><strong>h</strong>: height</li> <li><strong>w</strong>: width</li> </ul> <p>To effectively process this type of data, the <code class="language-plaintext highlighter-rouge">clstm3D</code> class will execute the cell for <code class="language-plaintext highlighter-rouge">t</code> time steps in the forward function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CLSTMCell3D</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CLSTMCell3D</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_channels</span> <span class="o">=</span> <span class="n">hidden_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>  <span class="c1"># to preserve spatial dimensions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv3d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span> <span class="o">+</span> <span class="n">hidden_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="mi">4</span> <span class="o">*</span> <span class="n">hidden_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">padding</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
        <span class="c1"># Concatenate input and hidden state along the channel dimension
</span>        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">conv_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="n">conv_output</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>  <span class="c1"># input gate
</span>        <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>  <span class="c1"># forget gate
</span>        <span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>  <span class="c1"># output gate
</span>        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>     <span class="c1"># cell gate
</span>
        <span class="n">c_next</span> <span class="o">=</span> <span class="n">f</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">h_next</span> <span class="o">=</span> <span class="n">o</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">c_next</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_next</span><span class="p">,</span> <span class="n">c_next</span>

<span class="k">class</span> <span class="nc">CLSTM3D</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">ConvLSTM module for 3D data.</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CLSTM3D</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm_cells</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">CLSTMCell3D</span><span class="p">(</span><span class="n">input_dim</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Initialize hidden and cell states
</span>        <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">t_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t_step</span><span class="p">]</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_first</span> <span class="k">else</span> <span class="n">x</span><span class="p">[</span><span class="n">t_step</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">lstm_cell</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lstm_cells</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">),</span>
                                                <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">))</span>
                <span class="n">hidden_state</span><span class="p">,</span> <span class="n">cell_state</span> <span class="o">=</span> <span class="nf">lstm_cell</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">cell_state</span><span class="p">)</span>
                <span class="n">x_t</span> <span class="o">=</span> <span class="n">hidden_state</span>
            <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="implementation-of-clstm-in-unet3d">Implementation of CLSTM in UNet3D</h2> <p>For more details on the implementation of CLSTM in 3D, you can check my project posting <a href="https://subinek.com/projects/4_project/">4D Time-Resolved UNet</a>.</p> <p>The code for the CLSTM cell is open-sourced and can be found on <a href="https://github.com/eksubin/CLSTM-Unet4D/blob/master/utils.py">GitHub</a>.</p> <p>You can also explore the full implementation of the 4D time-resolved UNet in this <a href="https://github.com/eksubin/CLSTM-Unet4D/tree/master">GitHub repository</a>.</p> <h2 id="limitations">Limitations</h2> <p>One of the limitations of this approach is the larger memory footprint. LSTM cells are designed to store and manage higher-dimensional data, especially when dealing with 3D data over time. The complete implementation of the 4D time-resolved UNet was able to run on an A30 GPU with 40 GB VRAM, albeit with a lower patch size.</p>]]></content><author><name></name></author><category term="Neural"/><category term="Network"/><category term="AI"/><category term="Segmentation"/><summary type="html"><![CDATA[Tutorial on how to build a convolutional LSTM for 4D Data]]></summary></entry></feed>