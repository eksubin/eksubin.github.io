<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://eksubin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://eksubin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-16T12:14:53+00:00</updated><id>https://eksubin.github.io/feed.xml</id><title type="html">blank</title><subtitle>Portfolio of Mr Subin Erattakulangara </subtitle><entry><title type="html">Docker for researchers</title><link href="https://eksubin.github.io/blog/2025/docker/" rel="alternate" type="text/html" title="Docker for researchers"/><published>2025-02-11T00:00:00+00:00</published><updated>2025-02-11T00:00:00+00:00</updated><id>https://eksubin.github.io/blog/2025/docker</id><content type="html" xml:base="https://eksubin.github.io/blog/2025/docker/"><![CDATA[<div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/docker-gemini-480.webp 480w,/assets/img/docker-gemini-800.webp 800w,/assets/img/docker-gemini-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/docker-gemini.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Images generated using ImageGen3 </div> <p>The blue whale logo of the docker is a familiar face we have seen so frequntly in the web, Eventhough many of us have seen it, majority of us don’t use this technology while sharing your source code related to research. My duty here is to convince to you use docker also to provide an easy to use totorial that you can use to package your code. First let’s understand the major issues on reproducing results from a code.</p> <h3 id="major-issues-with-code-reproducibility">Major Issues with Code Reproducibility</h3> <ol> <li> <p><strong>Dependencies, Dependencies, Dependencies</strong> - To run your code, you’ll likely rely on numerous Python libraries and their dependencies. As you may know, different versions of these libraries can behave differently. For instance, if you developed a piece of code five years ago, you might need the exact versions of the libraries that were available at that time.</p> <p>So, how can we address this issue?</p> <p>It’s simple: create a <code class="language-plaintext highlighter-rouge">requirements.txt</code> file specifying the exact versions of the libraries you used.</p> <p>But how many of us actually do this? Even if you manage to create such a file, there’s still the possibility that <code class="language-plaintext highlighter-rouge">pip</code> may struggle to find the required versions if they are outdated.</p> </li> <li> <p><strong>Laziness</strong> - This can also be considered a significant issue. Setting up an environment to run a new set of codes downloaded from GitHub can be quite demanding. As a result, many researchers may hesitate to attempt it. Instead, they might just read the results published in journals and feel satisfied without verifying or reproducing the findings themselves.</p> </li> <li> <p><strong>OS Compatibility</strong> - One of the challenges in code reproducibility arises from differences in operating systems. Code that runs smoothly on one OS may encounter issues on another due to variations in system libraries, file paths, and default settings. For example, a piece of software that works perfectly on Windows might crash or behave unexpectedly on Linux.</p> <p>This is where Docker shines. By containerizing your applications, Docker creates an environment that encapsulates all necessary dependencies and system configurations. This means that regardless of the host operating system—be it Windows, macOS, or Linux—your code will run consistently as long as Docker is installed.</p> <p>Using Docker, researchers can create a uniform environment for their work, eliminating the guesswork that comes with OS differences. With a simple command, anyone can pull your Docker image and spin up an identical environment to run your code, ensuring that they will observe the same results as you did. This minimizes the “it works on my machine” problem and significantly enhances the reproducibility of computational research.</p> </li> </ol> <h2 id="key-concepts">Key Concepts</h2> <ul> <li><strong>Image</strong>: A read-only template with instructions to create a container.</li> <li><strong>Container</strong>: A runnable instance of an image.</li> <li><strong>Dockerfile</strong>: A text file with instructions to build a Docker image.</li> <li><strong>Docker Hub</strong>: A public registry that stores Docker images.</li> </ul> <hr/> <h2 id="common-docker-commands">Common Docker Commands</h2> <h3 id="docker-version--info">Docker Version &amp; Info</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nt">--version</span>         <span class="c"># Show the Docker version installed</span>
docker info              <span class="c"># Display system-wide information about Docker</span>
</code></pre></div></div> <h3 id="managing-images">Managing Images</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker pull &lt;image&gt;      <span class="c"># Download an image from Docker Hub</span>
docker images            <span class="c"># List all images</span>
docker rmi &lt;image&gt;       <span class="c"># Remove an image</span>
docker build <span class="nt">-t</span> &lt;name&gt; <span class="nb">.</span> <span class="c"># Build an image from a Dockerfile in the current directory</span>
</code></pre></div></div> <h3 id="managing-containers">Managing containers</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run &lt;image&gt;                   <span class="c"># Run a container from an image</span>
docker run <span class="nt">-d</span> &lt;image&gt;                <span class="c"># Run a container in detached mode (background)</span>
docker run <span class="nt">-it</span> &lt;image&gt; bash          <span class="c"># Run a container interactively with a bash shell</span>
docker stop &lt;container&gt;              <span class="c"># Stop a running container</span>
docker start &lt;container&gt;             <span class="c"># Start a stopped container</span>
docker restart &lt;container&gt;           <span class="c"># Restart a container</span>
docker <span class="nb">rm</span> &lt;container&gt;                <span class="c"># Remove a stopped container</span>
docker ps                            <span class="c"># List running containers</span>
docker ps <span class="nt">-a</span>                         <span class="c"># List all containers (running and stopped)</span>
</code></pre></div></div> <h3 id="managing-volumes">Managing volumes</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker volume create &lt;volume_name&gt;   <span class="c"># Create a volume</span>
docker volume <span class="nb">ls</span>                     <span class="c"># List all volumes</span>
docker volume <span class="nb">rm</span> &lt;volume_name&gt;       <span class="c"># Remove a volume</span>
docker run <span class="nt">-v</span> &lt;volume_name&gt;:/path/in/container &lt;image&gt; <span class="c"># Attach volume to container</span>
</code></pre></div></div> <h3 id="networks">Networks</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network <span class="nb">ls</span>                    <span class="c"># List networks</span>
docker network create &lt;network_name&gt; <span class="c"># Create a network</span>
docker network connect &lt;network_name&gt; &lt;container&gt; <span class="c"># Connect a container to a network</span>
docker network disconnect &lt;network_name&gt; &lt;container&gt; <span class="c"># Disconnect a container from a network</span>
</code></pre></div></div> <h3 id="docker-file-basics">Docker file basics</h3> <p>A Dockerfile contains instructions to build an image. Here’s an example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Start with a base image</span>
FROM python:3.8

<span class="c"># Set working directory</span>
WORKDIR /app

<span class="c"># Copy files to the container</span>
COPY <span class="nb">.</span> /app

<span class="c"># Install dependencies</span>
RUN pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="c"># Run the application</span>
CMD <span class="o">[</span><span class="s2">"python"</span>, <span class="s2">"app.py"</span><span class="o">]</span>
</code></pre></div></div> <h3 id="build-and-run-an-image">Build and run an image</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> myapp <span class="nb">.</span>       <span class="c"># Build Docker image from Dockerfile</span>
docker run <span class="nt">-p</span> 8080:80 myapp   <span class="c"># Run container and map ports (host:container)</span>
</code></pre></div></div> <h3 id="docker-compose">Docker Compose</h3> <p>Docker Compose allows you to define and manage multi-container Docker applications. docker-compose.yml example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: <span class="s1">'3'</span>
services:
  web:
    image: nginx
    ports:
      - <span class="s2">"8080:80"</span>
  db:
    image: postgres
    environment:
      POSTGRES_USER: example
      POSTGRES_PASSWORD: example
</code></pre></div></div> <p>Docker Compose Commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up       <span class="c"># Start all services in the docker-compose.yml</span>
docker-compose down     <span class="c"># Stop and remove services</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Tools"/><category term="AI"/><category term="Docker"/><summary type="html"><![CDATA[Tutorial on how to use docker for reproducable computational research]]></summary></entry><entry><title type="html">MonaiLabelLite - A lightweight implementation of MonaiLabel for Docker</title><link href="https://eksubin.github.io/blog/2025/2025/" rel="alternate" type="text/html" title="MonaiLabelLite - A lightweight implementation of MonaiLabel for Docker"/><published>2025-02-11T00:00:00+00:00</published><updated>2025-02-11T00:00:00+00:00</updated><id>https://eksubin.github.io/blog/2025/2025</id><content type="html" xml:base="https://eksubin.github.io/blog/2025/2025/"><![CDATA[<div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/docker_minimization.webp" sizes="95vw"/> <img src="/assets/img/docker_minimization.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Monai Label is an excellent tool, widely regarded as one of the best available for developers and researchers working on deep learning applications in medical imaging. However, the recent additions to the Monai Label repository have significantly increased its size and complexity.</p> <ul> <li>This increase generally doesn’t impact users running the software locally or utilizing a GPU on their system.</li> <li>It also adds considerable value to the open-source community.</li> </ul> <h2 id="impact-on-building-a-docker-image">Impact on Building a Docker Image</h2> <p>When building Docker images, we strive for a lighter footprint. Unfortunately, integrating the latest Monai Label into your code can lead to substantial image bloat. Personally, I sought to deploy a Monai server on CPU for a custom segmentation algorithm I developed. Yet, with the latest version of Monai Label, my Docker image ballooned to 20 GB, causing performance issues on MacBooks due to the increased overhead of running Docker Desktop. Thus, I decided to create a streamlined version of Monai Label tailored for CPU usage.</p> <p>Here are some components you can remove to achieve a lighter image:</p> <ol> <li>The SAM2 model is included by default for all applications when Python is 3.10 or higher. You can remove this if SAM functionality is not required for your code.</li> <li>If you’re not performing inference on a GPU, the GPU version of PyTorch can be omitted.</li> <li>Nvidia CUDA setup files can take up to 500 MB of space and can be excluded if not needed.</li> </ol> <p>After eliminating these components and ensuring that the CPU version of libraries like PyTorch is installed, I managed to reduce the Docker image size from <strong>20 GB to 10 GB</strong>.</p> <h2 id="my-approach-to-reducing-image-size">My Approach to Reducing Image Size</h2> <ol> <li>Created a forked repository of the original Monai Label.</li> <li>Removed SAM installation from <code class="language-plaintext highlighter-rouge">requirements.txt</code> to save storage.</li> <li>Executed the <code class="language-plaintext highlighter-rouge">requirements.txt</code> independently, which unfortunately installs the GPU version of PyTorch.</li> <li>Utilized a wheel to reinstall PyTorch and torchvision with CPU support.</li> <li>Cloned the Git repository and added it to the root path.</li> </ol> <h2 id="docker-code">Docker code</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">FROM</span> <span class="n">python</span><span class="p">:</span><span class="mf">3.10</span><span class="o">-</span><span class="n">slim</span>

<span class="n">WORKDIR</span> <span class="o">/</span><span class="n">app</span>

<span class="c1"># Install git
</span><span class="n">RUN</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">update</span> <span class="o">&amp;&amp;</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="o">-</span><span class="n">y</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">install</span><span class="o">-</span><span class="n">recommends</span> <span class="n">git</span> <span class="o">&amp;&amp;</span> <span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">apt</span><span class="o">/</span><span class="n">lists</span><span class="o">/*</span>

<span class="c1"># Clone MONAI Label Lite - No installation of SAM
</span><span class="n">RUN</span> <span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">eksubin</span><span class="o">/</span><span class="n">MONAILabelLite</span>

<span class="c1"># Install Python dependencies
</span><span class="n">RUN</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">cache</span><span class="o">-</span><span class="nb">dir</span> <span class="o">-</span><span class="n">r</span> <span class="n">MONAILabelLite</span><span class="o">/</span><span class="n">requirements</span><span class="p">.</span><span class="n">txt</span>

<span class="c1"># Set MONAILabel script paths in environment variables
</span><span class="n">ENV</span> <span class="n">PATH</span><span class="o">=</span><span class="sh">"</span><span class="s">/app/MONAILabelLite/monailabel/scripts:${PATH}</span><span class="sh">"</span>

<span class="c1"># Remove torch gpu and install torch cpu
</span><span class="n">RUN</span> <span class="n">pip</span> <span class="n">uninstall</span> <span class="o">-</span><span class="n">y</span> <span class="n">torch</span> <span class="n">torchvision</span>
<span class="n">RUN</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span> <span class="n">torchvision</span> <span class="o">--</span><span class="n">extra</span><span class="o">-</span><span class="n">index</span><span class="o">-</span><span class="n">url</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">download</span><span class="p">.</span><span class="n">pytorch</span><span class="p">.</span><span class="n">org</span><span class="o">/</span><span class="n">whl</span><span class="o">/</span><span class="n">cpu</span>

<span class="c1"># Remove the git - Not needed after the first operation
</span><span class="n">RUN</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">update</span> <span class="o">&amp;&amp;</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">remove</span> <span class="o">-</span><span class="n">y</span> <span class="n">git</span>

<span class="c1"># Copy the application code
</span><span class="n">COPY</span> <span class="p">.</span> <span class="p">.</span>
<span class="c1"># Expose the port the app runs on
</span><span class="n">EXPOSE</span> <span class="mi">8000</span>

<span class="c1"># Command to run the application
</span><span class="n">CMD</span> <span class="p">[</span><span class="sh">"</span><span class="s">add command to run your application</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Tools"/><category term="AI"/><category term="Docker"/><summary type="html"><![CDATA[Latest Monai Label is too heavy for a docker image, Let's make it lighter]]></summary></entry><entry><title type="html">Building a CLSTM cell for 4D Data</title><link href="https://eksubin.github.io/blog/2025/code/" rel="alternate" type="text/html" title="Building a CLSTM cell for 4D Data"/><published>2025-01-26T00:00:00+00:00</published><updated>2025-01-26T00:00:00+00:00</updated><id>https://eksubin.github.io/blog/2025/code</id><content type="html" xml:base="https://eksubin.github.io/blog/2025/code/"><![CDATA[<p>LSTM or Long Short-Term Memory is a specialized type of recurrent neural network (RNN) designed to address the vanishing gradient problem often encountered by traditional RNNs when processing long sequences.</p> <p>Recurrent Neural Networks (RNNs) are a type of neural network designed to work with sequential data. Unlike traditional feedforward networks, RNNs have connections that loop back on themselves, allowing them to maintain a “memory” of past inputs. This memory enables them to process information from previous steps in the sequence, making them well-suited for tasks like natural language processing, speech recognition, and time series analysis. However they are prone to vanishing gradient problem.</p> <p>LSTMs overcome this by incorporating a memory cell, which allows the network to store and access information over extended periods. Here is a pictoral representation of an LSTM Cell.</p> <div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/clstmCell-480.webp 480w,/assets/img/clstmCell-800.webp 800w,/assets/img/clstmCell-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/clstmCell.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Comparison between RNN and CLSTM Cell </div> <p>The LSTM cell consists of three gates:</p> <ul> <li><strong>Input Gate</strong>: Determines which new information should be stored in the cell.</li> <li><strong>Forget Gate</strong>: Decides which information should be discarded from the cell.</li> <li><strong>Output Gate</strong>: Controls what information from the cell is used to compute the output.</li> </ul> <p>These gates regulate the flow of information into and out of the cell, enabling the network to selectively remember or forget information.</p> <h2 id="pytorch-implementation">Pytorch implementation</h2> <p>Here is the pytorch implementation of a vanilla LSTM Cell.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">LSTMCell</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LSTMCell</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="c1"># Forget, Input, and Output gates weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Wf</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wi</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wo</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hx</span><span class="p">):</span>
        <span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span> <span class="o">=</span> <span class="n">hx</span>

        <span class="c1"># Concatenate input and previous hidden state
</span>        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Forget gate
</span>        <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wf</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Input gate
</span>        <span class="n">i_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wi</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Candidate memory
</span>        <span class="n">c_hat_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wc</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Cell state
</span>        <span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">c_hat_t</span>
        <span class="c1"># Output gate
</span>        <span class="n">o_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wo</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Hidden state
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span>
</code></pre></div></div> <p>Sample code for running the vanilla LSTM Cell.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">lstm_cell</span> <span class="o">=</span> <span class="nc">LSTMCell</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

<span class="c1"># Example input (batch_size=1, input_size=4)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
<span class="c1"># Initial hidden state and cell state
</span><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">c_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

<span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span> <span class="o">=</span> <span class="nf">lstm_cell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next hidden state:</span><span class="sh">"</span><span class="p">,</span> <span class="n">h_t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next cell state:</span><span class="sh">"</span><span class="p">,</span> <span class="n">c_t</span><span class="p">)</span>
</code></pre></div></div> <h3 id="converting-lstm-into-clstm">Converting LSTM into CLSTM</h3> <p>To convert an LSTM cell into a Convolutional LSTM (ConvLSTM), you typically use convolutional layers instead of fully connected layers for processing the input and hidden states. Below is an implementation of a ConvLSTM cell in PyTorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">ConvLSTMCell</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ConvLSTMCell</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>

        <span class="c1"># Define the convolutional layers for the gates
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Wf</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wi</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wo</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hx</span><span class="p">):</span>
        <span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span> <span class="o">=</span> <span class="n">hx</span>

        <span class="c1"># Concatenate input and previous hidden state along the channel dimension
</span>        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Forget gate
</span>        <span class="n">f_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wf</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Input gate
</span>        <span class="n">i_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wi</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Candidate memory
</span>        <span class="n">c_hat_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wc</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Now the operation is a Convolution
</span>        <span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">c_hat_t</span>
        <span class="c1"># Output gate
</span>        <span class="n">o_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wo</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="c1"># Hidden state
</span>        <span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span>
</code></pre></div></div> <p>Sample code to use the CLSTM cell</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_channels</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">hidden_channels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">conv_lstm_cell</span> <span class="o">=</span> <span class="nc">ConvLSTMCell</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">,</span>
                                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Padding to keep the output spatial dimensions same
</span>
<span class="c1"># Example input (batch_size=1, channels=1, height=4, width=4)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1"># Initial hidden state and cell state
</span><span class="n">h_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">c_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span> <span class="o">=</span> <span class="nf">conv_lstm_cell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next hidden state shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">h_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Next cell state shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">c_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h2 id="preparing-clstm-for-handling-3d-data">Preparing CLSTM for Handling 3D Data</h2> <p>When dealing with 3D data over time, the input shape follows the format:</p> <ul> <li><strong>b</strong>: batch size</li> <li><strong>t</strong>: number of time frames</li> <li><strong>c</strong>: number of channels</li> <li><strong>d</strong>: depth</li> <li><strong>h</strong>: height</li> <li><strong>w</strong>: width</li> </ul> <p>To effectively process this type of data, the <code class="language-plaintext highlighter-rouge">clstm3D</code> class will execute the cell for <code class="language-plaintext highlighter-rouge">t</code> time steps in the forward function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CLSTMCell3D</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CLSTMCell3D</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_channels</span> <span class="o">=</span> <span class="n">hidden_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>  <span class="c1"># to preserve spatial dimensions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv3d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span> <span class="o">+</span> <span class="n">hidden_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="mi">4</span> <span class="o">*</span> <span class="n">hidden_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">padding</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
        <span class="c1"># Concatenate input and hidden state along the channel dimension
</span>        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">conv_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="n">conv_output</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>  <span class="c1"># input gate
</span>        <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>  <span class="c1"># forget gate
</span>        <span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>  <span class="c1"># output gate
</span>        <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>     <span class="c1"># cell gate
</span>
        <span class="n">c_next</span> <span class="o">=</span> <span class="n">f</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">h_next</span> <span class="o">=</span> <span class="n">o</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">c_next</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_next</span><span class="p">,</span> <span class="n">c_next</span>

<span class="k">class</span> <span class="nc">CLSTM3D</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">ConvLSTM module for 3D data.</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">CLSTM3D</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm_cells</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">CLSTMCell3D</span><span class="p">(</span><span class="n">input_dim</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Initialize hidden and cell states
</span>        <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">t_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t_step</span><span class="p">]</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_first</span> <span class="k">else</span> <span class="n">x</span><span class="p">[</span><span class="n">t_step</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">lstm_cell</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lstm_cells</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">),</span>
                                                <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">))</span>
                <span class="n">hidden_state</span><span class="p">,</span> <span class="n">cell_state</span> <span class="o">=</span> <span class="nf">lstm_cell</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">hidden_states</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">cell_state</span><span class="p">)</span>
                <span class="n">x_t</span> <span class="o">=</span> <span class="n">hidden_state</span>
            <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="implementation-of-clstm-in-unet3d">Implementation of CLSTM in UNet3D</h2> <p>For more details on the implementation of CLSTM in 3D, you can check my project posting <a href="https://subinek.com/projects/4_project/">4D Time-Resolved UNet</a>.</p> <p>The code for the CLSTM cell is open-sourced and can be found on <a href="https://github.com/eksubin/CLSTM-Unet4D/blob/master/utils.py">GitHub</a>.</p> <p>You can also explore the full implementation of the 4D time-resolved UNet in this <a href="https://github.com/eksubin/CLSTM-Unet4D/tree/master">GitHub repository</a>.</p> <h2 id="limitations">Limitations</h2> <p>One of the limitations of this approach is the larger memory footprint. LSTM cells are designed to store and manage higher-dimensional data, especially when dealing with 3D data over time. The complete implementation of the 4D time-resolved UNet was able to run on an A30 GPU with 40 GB VRAM, albeit with a lower patch size.</p>]]></content><author><name></name></author><category term="Neural"/><category term="Network"/><category term="AI"/><category term="Segmentation"/><summary type="html"><![CDATA[Tutorial on how to build a convolutional LSTM for 4D Data]]></summary></entry></feed>